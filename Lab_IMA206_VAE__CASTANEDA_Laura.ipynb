{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdpPHz2Lp6VD"
      },
      "source": [
        "# IMA 206 - Coding autoencoders in Pytorch\n",
        "\n",
        "The lab was originally created by Alasdair Newson (https://sites.google.com/site/alasdairnewson/)\n",
        "\n",
        "The current version is made by Loic Le Folgoc. If you have questions, please contact me at loic dot lefolgoc at telecom-paris dot fr.\n",
        "\n",
        "## Objective:\n",
        "\n",
        "The goal of this TP is to explore autoencoders and variational autoencoders applied to a simple dataset. In this first part, we will look at an autoencoder applied to MNIST. We recall that an autoencoder is a neural network with the following general architecture:\n",
        "\n",
        "\n",
        "![AUTOENCODER](https://drive.google.com/uc?id=11dfNujSHa2-_eThp2aTpL1M_hLaEQX-G)\n",
        "\n",
        "\n",
        "The tensor $z$ in the middle of the network is called a __latent code__, and it belongs to the latent space. It is this latent space which is interesting in autoencoders (for image synthesis, editing, etc).\n",
        "\n",
        "## Your task:\n",
        "You need to add the missing parts in the code (parts between # --- START CODE HERE and # --- END CODE HERE or # FILL IN CODE or ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp13aVUQq1WX"
      },
      "source": [
        "First of all, let's load some packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JqNeIJ8Op8Ao"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import pdb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U6NKzRPlDKZp"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hyj5dj_eui9D"
      },
      "source": [
        "First, we load the mnist dataset. I find that training on the full training dataset `mnist_trainset` is fast enough even on CPU (5-10 minutes), but should you need it, we create a reduced trainset below.\n",
        "\n",
        "Feel free to train on `mnist_trainset_reduced` instead if you prefer (results might be of poorer quality). To do so, replace the argument `mnist_trainset` in the `torch.utils.data.DataLoader(...)` call creating `mnist_train_loader` in the cell below by `mnist_trainset_reduced` (and same for `mnist_testset` and `mnist_testset_reduced`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YPLKlPrufSk",
        "outputId": "376768cb-21e9-478d-cea6-f5444103a795"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# MNIST Dataset\n",
        "mnist_trainset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "mnist_testset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
        "\n",
        "#create data loader with smaller dataset size\n",
        "max_mnist_size = 5000\n",
        "mnist_trainset_reduced = torch.utils.data.random_split(mnist_trainset, [max_mnist_size, len(mnist_trainset)-max_mnist_size])[0]\n",
        "mnist_train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "# download test dataset\n",
        "max_mnist_size = 1000\n",
        "mnist_testset_reduced = torch.utils.data.random_split(mnist_testset, [max_mnist_size, len(mnist_testset)-max_mnist_size])[0]\n",
        "mnist_test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=batch_size, shuffle=False, drop_last=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7YhlBT2PN9I",
        "outputId": "3d465c07-cd26-4bf1-b758-d68a47db6b4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([60000, 28, 28])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mnist_trainset.data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-bkK4ktwfvC"
      },
      "source": [
        "# 1. Vanilla Autoencoder\n",
        "\n",
        "Now, we define our autoencoder model. The autoencoder class `AEModel` is made of an `Encoder` and a `Decoder`, which we create first. We will reuse the `Encoder` and `Decoder` classes when building our variational autoencoder model, and wrap them in a `VAEModel` instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jLa2-jQwxSI"
      },
      "source": [
        "We will use the following convolutional architectures :\n",
        "\n",
        "__Encoder__ :\n",
        "- Conv layer, 32 filters, 4x4 kernel, stride=2, padding=1; + ReLU\n",
        "- Conv layer, 32 filters, 4x4 kernel, stride=2, padding=0; + ReLU\n",
        "- Conv layer, 32 filters, 4x4 kernel, stride=2, padding=0; + ReLU\n",
        "- Flatten\n",
        "- Dense layer with 64 output neurons; + ReLU\n",
        "- Dense layer with `self.latent_dim*self.multiplier` output neurons.\n",
        "\n",
        "For the autoencoder, `self.multiplier=1` as the encoder outputs a `self.latent_dim`-dimensional latent code. For the variational autoencoder, we will set `self.multiplier=2` as the encoder will output `self.latent_dim`-dimensional mean and log-variance parameters of the Gaussian distribution $q_\\phi(z|x)$.\n",
        "\n",
        "__Decoder__ (the decoder of the AE and VAE are the same, they always outputs an image/probability map, given a code $z$ as input):\n",
        "- Dense layer with 64 output neurons; + ReLU\n",
        "- Dense layer with ??? output neurons; + ReLU\n",
        "- Reshape, to a `(C, H, W)` tensor with `C=32`, `H=???`, `W=???`.\n",
        "- Conv transpose layer, 32 filters, 4x4 kernel, stride=2, padding=0; +ReLU\n",
        "- Conv transpose layer, 32 filters, 4x4 kernel, stride=2, padding=0; +ReLU\n",
        "- Conv transpose layer, 1 filter, 4x4 kernel, stride=2, padding=1; +Sigmoid\n",
        "\n",
        "The number of output neurons of the second dense layer is exactly the number of input neurons in the first dense layer of the encoder (i.e., the number of values in the feature maps of the conv layer immediately before it).\n",
        "\n",
        "For the reshape operations, use the ```A.view(dim_1,dim_2,...)``` function, where ```A``` is your tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNH7ScylwKa-"
      },
      "source": [
        "__Hint for computing the number of neurons that are not given to you__: This [great resource](https://madebyollin.github.io/convnet-calculator/) lets you compute the size of the output tensor following any convolution layer depending on the input tensor shape and conv parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0-IyPUQkIR-U"
      },
      "outputs": [],
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, latent_dim=10, multiplier=1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # Layer parameters\n",
        "        self.latent_dim = latent_dim\n",
        "        self.multiplier = multiplier\n",
        "\n",
        "        # Shape at the end of conv3\n",
        "        self.reshape = 32*2*2\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 32, kernel_size=4, stride=2, padding=0)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=4, stride=2, padding=0)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.lin1 = nn.Linear(self.reshape, 64)\n",
        "        self.lin2 = nn.Linear(64, self.latent_dim*self.multiplier)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Pass the input image mini-batch through conv, linear layers and\n",
        "        non-linearities to output a (B,D,2) tensor where B is the mini-batch\n",
        "        size and D the latent dimension.\n",
        "        '''\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Convolutional layers with ReLu activations\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(batch_size, -1)\n",
        "\n",
        "        # Fully connected layer with ReLu activation\n",
        "        x = F.relu(self.lin1(x))\n",
        "\n",
        "        # Fully connected layer for code z, or mean and log-variance\n",
        "        z = self.lin2(x)\n",
        "\n",
        "        # The shape of the output tensor should be (B,D) if multiplier=1,\n",
        "        # where B is the batch size, and D the latent dimension.\n",
        "        # Otherwise it should be (B,D,multiplier)\n",
        "        if self.multiplier == 1:\n",
        "            z = z.view(batch_size, self.latent_dim)\n",
        "        else:\n",
        "            z = z.view(batch_size, self.latent_dim, self.multiplier)\n",
        "\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "U64AqYvpYpr6"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, latent_dim=10):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # Shape required to start transpose convs (copy paste from the encoder)\n",
        "        self.reshape = 32*2*2\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.lin1 = nn.Linear(latent_dim, 64)\n",
        "        self.lin2 = nn.Linear(64, self.reshape)\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.convT1 = nn.ConvTranspose2d(32, 32, kernel_size=4, stride=2, padding=0)\n",
        "        self.convT2 = nn.ConvTranspose2d(32, 32, kernel_size=4, stride=2, padding=0)\n",
        "        self.convT3 = nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        batch_size = z.size(0)\n",
        "\n",
        "        # Fully connected layers with ReLu activations\n",
        "        x = F.relu(self.lin1(z))\n",
        "        x = F.relu(self.lin2(x))\n",
        "\n",
        "        # Reshape\n",
        "        x = x.view(batch_size, 32, 2, 2)\n",
        "\n",
        "        # Convolutional layers with ReLu activations\n",
        "        x = F.relu(self.convT1(x))\n",
        "        x = F.relu(self.convT2(x))\n",
        "\n",
        "        # Final conv layer with sigmoid activation\n",
        "        x = torch.sigmoid(self.convT3(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmGyzVp4HVRJ"
      },
      "source": [
        "The autoencoder model itself is basically a wrapper around an `Encoder` and a `Decoder`. In the forward pass, the input images contained in the tensor `x` are passed through the `Encoder` to obtain the latent codes `z` then these codes are fed to the `Decoder` to produce the reconstructions `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "T9MYicH9Zf3o"
      },
      "outputs": [],
      "source": [
        "class AEModel(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        \"\"\"\n",
        "        Class which defines model and forward pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        latent_dim : int\n",
        "            Dimensionality of latent code.\n",
        "        \"\"\"\n",
        "        super(AEModel, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        self.encoder = Encoder(latent_dim, multiplier = 1)\n",
        "        self.decoder = Decoder(latent_dim)\n",
        "\n",
        "    def forward(self, x, mode='sample'):\n",
        "        \"\"\"\n",
        "        Forward pass of model, used for training or reconstruction.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Batch of data. Shape (batch_size, n_chan, height, width)\n",
        "\n",
        "        Outputs a dictionary containing:\n",
        "          codes - the latent codes corresponding to the input images\n",
        "          reconstructions - the images reconstructed by the autoencoder\n",
        "        \"\"\"\n",
        "\n",
        "        # z is the output of the encoder\n",
        "        z = self.encoder(x)\n",
        "\n",
        "        # Decode the samples to image space\n",
        "        y = self.decoder(z)\n",
        "\n",
        "        # Return everything:\n",
        "        return {\n",
        "            'reconstructions': y,\n",
        "            'codes': z\n",
        "            }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeWePstgIVBw"
      },
      "source": [
        "Next, we carefully create the reconstruction loss. It will be reused for the VAE loss later on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FIb4BDrInYy"
      },
      "source": [
        "The reconstruction loss translates a pixel-wise Bernoulli probabilistic model into a loss (`F.binary_cross_entropy`). It takes input images `data` and reconstructed probability maps `reconstructions` and computes the binary cross-entropy, from the two images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CWrQbhOnbv7y"
      },
      "outputs": [],
      "source": [
        "def reconstruction_loss(reconstructions, data):\n",
        "    \"\"\"\n",
        "    Calculates the reconstruction loss for a batch of data. I.e. negative\n",
        "    log likelihood.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : torch.Tensor\n",
        "        Input data (e.g. batch of images). Shape : (batch_size, 1,\n",
        "        height, width).\n",
        "\n",
        "    reconstructions : torch.Tensor\n",
        "        Reconstructed data. Shape : (batch_size, 1, height, width).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    loss : torch.Tensor\n",
        "        Binary cross entropy, AVERAGED over images in the batch but SUMMED over\n",
        "        pixel and channel.\n",
        "    \"\"\"\n",
        "    batch_size, n_chan, height, width = reconstructions.size()\n",
        "\n",
        "    # The pixel-wise loss is the binary cross-entropy, computed from\n",
        "    # reconstructions and data. It is summed over pixels and averaged across\n",
        "    # samples in the batch.\n",
        "    loss = F.binary_cross_entropy(reconstructions, data, reduction='sum') \n",
        "    loss = loss / batch_size\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqibefFRJHy_"
      },
      "source": [
        "### Training the vanilla autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQbHTXMLJODw"
      },
      "source": [
        "The training proceeds as usual. We instantiate a model, move it to the correct device, create an optimizer and write the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NyZcTZP3a_kc"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "latent_dim = 10\n",
        "\n",
        "learning_rate = 1e-3\n",
        "n_epoch = 5 # if running on GPU you can use more epochs (10 or more)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft_3txj0bZjO",
        "outputId": "13273cfd-eaeb-4d63-9d05-5b66ae5349a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oV40vRMQRoG1"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "ae_model = AEModel(latent_dim = latent_dim)\n",
        "ae_model = ae_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sbnKjRTDaynz"
      },
      "outputs": [],
      "source": [
        "# Use the AdamW optimizer, set the correct learning rate and weight_decay to 1e-4\n",
        "optimizer = optim.AdamW(ae_model.parameters(), lr=learning_rate, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "S2-phJydcICh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/938 [00:00<?, ?batch/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 938/938 [01:09<00:00, 13.48batch/s, loss=140]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Train Loss: 181.0531\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 938/938 [01:10<00:00, 13.36batch/s, loss=93.2]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 110.9874\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 938/938 [01:09<00:00, 13.45batch/s, loss=105] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Train Loss: 96.7354\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 938/938 [01:08<00:00, 13.61batch/s, loss=90.7]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Train Loss: 90.8753\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 938/938 [01:10<00:00, 13.22batch/s, loss=84.5]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Train Loss: 88.1403\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "ae_model.train()\n",
        "\n",
        "for epoch in range(0,n_epoch):\n",
        "  train_loss=0.0\n",
        "\n",
        "  with tqdm(mnist_train_loader, unit=\"batch\") as tepoch:\n",
        "    for data, labels in tepoch:\n",
        "      tepoch.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "      # Put data on correct device, GPU or CPU\n",
        "      data = data.to(device)\n",
        "\n",
        "      # Pass the input data through the model\n",
        "      predict = ae_model(data)\n",
        "      reconstructions = predict['reconstructions']\n",
        "\n",
        "      # Compute the AE loss\n",
        "      loss = reconstruction_loss(reconstructions, data)\n",
        "\n",
        "      # Backpropagate\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Aggregate the training loss for display at the end of the epoch\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # tqdm bar displays the loss\n",
        "      tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "  print('Epoch {}: Train Loss: {:.4f}'.format(epoch, train_loss/len(mnist_train_loader)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHscBN4KJ2Sq"
      },
      "source": [
        "### Testing the vanilla autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3EbmswSzJdK"
      },
      "source": [
        "We define functions for qualitative testing of the autoencoder model. We will reuse them throughout the lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "T8jXjdRyzMy2"
      },
      "outputs": [],
      "source": [
        "def display_images(imgs):\n",
        "  '''\n",
        "  Display a batch of images (typically synthetic/generated images)\n",
        "  '''\n",
        "  r = 1\n",
        "  c = imgs.shape[0]\n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  for j in range(c):\n",
        "    # black and white images\n",
        "    axs[j].imshow(imgs[j, 0,:,:].detach().cpu().numpy(), cmap='gray')\n",
        "    axs[j].axis('off')\n",
        "  plt.show()\n",
        "\n",
        "def display_ae_images(ae_model, test_imgs):\n",
        "  '''\n",
        "  Display a batch of input images along with their reconstructions by a given model\n",
        "    First row: input images\n",
        "    Second row: reconstructed images\n",
        "  '''\n",
        "  n_images = 5\n",
        "  idx = np.random.randint(0, test_imgs.shape[0], n_images)\n",
        "  test_imgs = test_imgs[idx,:,:,:]\n",
        "\n",
        "  # get output images\n",
        "  output_imgs = ae_model(test_imgs.to(ae_model.encoder.conv1.weight.device))['reconstructions']\n",
        "  output_imgs = output_imgs.detach().cpu().numpy()\n",
        "\n",
        "  r = 2\n",
        "  c = n_images\n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  for j in range(c):\n",
        "    axs[0,j].imshow(test_imgs[j, 0,:,:], cmap='gray')\n",
        "    axs[0,j].axis('off')\n",
        "    axs[1,j].imshow(output_imgs[j, 0,:,:], cmap='gray')\n",
        "    axs[1,j].axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQiEplCiKnlT"
      },
      "source": [
        "Let's see how well the autoencoder reconstructs images from the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9pbXch29d68D"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# reconstructing training images\n",
        "train_imgs = next(iter(mnist_train_loader))[0]\n",
        "display_ae_images(ae_model, train_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3XxWKjGKtoA"
      },
      "source": [
        "What about images from the test set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ExMLThLofLn2"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# reconstructing test images\n",
        "test_imgs = next(iter(mnist_test_loader))[0]\n",
        "display_ae_images(ae_model, test_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icWAOpmWKwid"
      },
      "source": [
        "There is not too much overfitting at work here apparently. We can quantify this by computing the reconstruction loss over the test dataset (below) and comparing it to the reconstruction loss over the training dataset at the end of training (check the training cell above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HbyzjCqUjHQU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 157/157 [00:05<00:00, 28.43batch/s, loss=81.8]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 86.3796\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "ae_model.eval()\n",
        "test_loss=0.0\n",
        "\n",
        "# We will store all the latent codes corresponding to the test images for reuse\n",
        "# later on.\n",
        "zs_test = np.zeros((len(mnist_test_loader.dataset),ae_model.latent_dim))\n",
        "\n",
        "n = 0\n",
        "with tqdm(mnist_test_loader, unit=\"batch\") as tepoch:\n",
        "  for data, labels in tepoch:\n",
        "    # Put the data on the correct device:\n",
        "    data = data.to(device)\n",
        "\n",
        "    # Pass the data through the model\n",
        "    predict = ae_model(data)\n",
        "    reconstructions = predict['reconstructions']\n",
        "    z = predict['codes']\n",
        "\n",
        "    # Compute the AE loss\n",
        "    loss = reconstruction_loss(reconstructions, data)\n",
        "\n",
        "    # Store quantities of interest\n",
        "    minibatch_size = z.shape[0]\n",
        "    zs_test[n:(n+minibatch_size),:] = z.detach().cpu().numpy()\n",
        "\n",
        "    # Compute the loss\n",
        "    test_loss += loss.item()\n",
        "\n",
        "    # tqdm bar displays the loss\n",
        "    tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "    # increment n to fill next parts of the arrays\n",
        "    n += minibatch_size\n",
        "\n",
        "print('Test Loss: {:.4f}'.format(test_loss/len(mnist_test_loader)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "assPaJqB5sa-"
      },
      "source": [
        "The test and training average reconstruction losses are indeed similar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoLTYl8ELZsj"
      },
      "source": [
        "Are you happy with the quality of the __reconstructions__? Next, we will see if this autoencoder model is good at __generating__ images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO9ATQEiyr3b"
      },
      "source": [
        "# 2. Image generation with the vanilla autoencoder\n",
        "\n",
        "Unfortunately, the vanilla autoencoder is not in itself a generative model because it does not define a joint probability distribution of the data and latent codes. We need to come up with roundabout ways to synthetize data based on this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECzGyqbyMaCt"
      },
      "source": [
        "In this section, we consider two naïve approaches to creating generative models from the AE. The general idea is the following:\n",
        "\n",
        "- train an autoencoder\n",
        "- estimate different statistics (mean, variance) of the data in the latent space\n",
        "- using these statistics, define a model based on a Gaussian distribution in the latent space\n",
        "- generate latent codes with this distribution, then decode them back to image space to obtain synthetic images\n",
        "\n",
        "We will consider these two situations :\n",
        "\n",
        "- a multivariate Gaussian distribution with __diagonal covariance matrix__ (each latent variable is an independent random variable). This requires the mean and variance in each latent variable;\n",
        "- a multivariate Gaussian distribution with __non-diagonal covariance matrix__. This requires the mean and covariance matrix of the latent codes.\n",
        "\n",
        "Obviously, since this is done _a posteriori_ after training the autoencoder, there is nothing which guarantees that the latent codes do indeed follow a Gaussian distribution. Our goal will be to verify that Variational Autoencoders indeed produce better results than such naïve approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2M1-BRmf56d"
      },
      "source": [
        "### 2.0. Defining and generating random Gaussian latent codes\n",
        "\n",
        "Let $z$ be a latent code and $D$ the dimension of the latent space (called ``latent_dim`` in the code). We suppose that the $z$'s follow a multivariate Gaussian distribution, written as:\n",
        "\n",
        "\\begin{equation}\n",
        "z \\sim \\mathcal{N}\\left(\n",
        "\\mu,\n",
        "\\bf{C}\n",
        "\\right),\n",
        "\\end{equation}\n",
        "where $\\mu$ and $\\bf{C}$ are the mean vector and covariance matrix of the Gaussian distribution. To define such a generative model, we must therefore determine $\\mu$ and $\\bf{C}$. Once this is done, we can generate a random Gaussian latent code in the following manner:\n",
        "\n",
        "\\begin{equation}\n",
        "z = \\mu + {\\bf{L}} \\varepsilon,\n",
        "\\end{equation}\n",
        "where $\\varepsilon$ is a random vector drawn from a multivariate normal distribution ($\\mu=0$ and ${\\bf{C}} = \\text{Id}$), and $\\bf{L}$ is output by a Cholesky decomposition of the positive semi-definite covariance matrix. In other words:\n",
        "\n",
        "\\begin{equation}\n",
        "{\\bf{C}} = {\\bf{L}}{\\bf{L}^T}.\n",
        "\\end{equation}\n",
        "\n",
        "This gives a simple method of producing a multivariate Gaussian random variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWpucm972i7j"
      },
      "source": [
        "### 2.1. A Gaussian model with diagonal covariance\n",
        "\n",
        "The first naïve model is  defined in this first case as:\n",
        "\n",
        "- $\\bf{\\mu}=\\left[\\mu_0, \\mu_1, \\cdots, \\mu_{d-1}\\right]^T$\n",
        "- $\n",
        "  \\bf{C} = \\begin{pmatrix}\n",
        "\\sigma_0^2 & 0 & \\cdots & 0 \\\\\n",
        "0 & \\sigma_1^2 & \\cdots & 0 \\\\\n",
        "\\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\cdots & \\sigma_{d-1}^2\n",
        "\\end{pmatrix}$\n",
        "\n",
        "In this situation, therefore, the matrix $\\bf{L}$ can be calculated quite simply, as:\n",
        "- $\n",
        "  \\bf{L} = \\begin{pmatrix}\n",
        "\\sigma_0 & 0 & \\cdots & 0 \\\\\n",
        "0 & \\sigma_1 & \\cdots & 0 \\\\\n",
        "\\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\cdots & \\sigma_{d-1}\n",
        "\\end{pmatrix}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZCIY7RsN4NY"
      },
      "source": [
        "We are going to compute the mean and the component-wise standard deviations from a batch of data. For simplicity you are going to use the latent codes `zs_test` corresponding to the test data to estimate these quantities.<br>\n",
        "\n",
        "It is actually bad practice, and it would be better to estimate them from the training dataset. We do not do so here for convenience because we have already computed `zs_test` above (we have verified above that overfitting was not a problem, so the difference between the two estimates should be minor)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sUXHCtvW2iQ0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average of latent codes: [-2.90463476 19.56843675 -2.11344441 14.93470955  2.65796819  4.17699941\n",
            " -9.00254967  2.38892447 -2.84102682 -4.73619446]\n",
            "Standard deviation of latent codes: [4.84419548 5.51011819 5.1175305  5.79268413 5.00926134 6.35075825\n",
            " 3.90786357 5.87128584 4.72921461 6.88132048]\n"
          ]
        }
      ],
      "source": [
        "# zs_test is of shape (N,D) where N is the test dataset size and D the latent dimension\n",
        "# Compute the vector of mean values and the vector of component-wise std's.\n",
        "z_average = zs_test.mean(axis=0)\n",
        "z_sigma = zs_test.std(axis=0)\n",
        "\n",
        "print(\"Average of latent codes:\",z_average)\n",
        "print(\"Standard deviation of latent codes:\",z_sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lrpc62ML9K4l"
      },
      "source": [
        "Now, in the next cell generate data with this simple generative model using the approach described above. Display these images with the `display_images` function.\n",
        "\n",
        "__Hint__. You do not actually have to define the matrix $\\bf{L}$ in this case, an element-wise multiplication of two (properly chosen) vectors will suffice. To generate multivariate normal random variables you can use the following Pytorch function:\n",
        "\n",
        "- `torch.randn`\n",
        "\n",
        "To convert a numpy array to pytorch tensor, use `torch.from_numpy(...).float()`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "cRXyUkVeppif"
      },
      "outputs": [],
      "source": [
        "def generate_images_diagonal_gaussian(ae_model, z_average, z_sigma, n_images = 5):\n",
        "    \n",
        "    # Sample noise from a standard Gaussian distribution\n",
        "    epsilon = torch.randn(n_images, z_average.shape[0])\n",
        "\n",
        "    # Using epsilon, generate samples from N(mu,C)\n",
        "    z_average = torch.from_numpy(z_average).float()\n",
        "    z_sigma = torch.from_numpy(z_sigma).float()\n",
        "    z_generated = z_average + z_sigma*epsilon\n",
        "\n",
        "\n",
        "    # Decode back to image space\n",
        "    imgs_generated = ae_model.decoder(z_generated.to(device))\n",
        "\n",
        "    return imgs_generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1_Tekii-9QEo"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "imgs_generated = generate_images_diagonal_gaussian(ae_model, z_average, z_sigma, n_images=5)\n",
        "display_images(imgs_generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiNaEgLIloeA"
      },
      "source": [
        "What do you think of these samples? Next let's try a slightly more sophisticated model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjVPfkRKYMSh"
      },
      "source": [
        "### 2.2. Non-diagonal Gaussian model\n",
        "\n",
        "The second model uses a non-diagonal covariance matrix $\\bf{C}$ in the multivariate Gaussian distribution. In the next cell, calculate the mean and covariance matrix from `zs_test`.\n",
        "\n",
        "__Hint__. You can use the `np.cov` function. Make sure to put the data in the right format for this. The documentation might be incorrect, print the shape of z_covariance to verify that you have a matrix of the correct shape (the covariance matrix and not the Gram matrix)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ArXgre39CD2H"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average of latent codes: [-2.90463476 19.56843675 -2.11344441 14.93470955  2.65796819  4.17699941\n",
            " -9.00254967  2.38892447 -2.84102682 -4.73619446]\n",
            "Covariance matrix of latent codes: [[ 23.46857672   4.96158103  -9.26911465  -8.20242808   0.88129497\n",
            "    9.93497783  -4.1667451   11.77262839  -2.45207175  -1.69033975]\n",
            " [  4.96158103  30.36443892 -12.34423037   9.05286833  -9.42206278\n",
            "   10.01067292   2.22609494   2.24571014   6.63538105  -6.7975317 ]\n",
            " [ -9.26911465 -12.34423037  26.19173763   2.34141776   2.55010774\n",
            "   -2.19056391   1.24388597  -5.38700775  -5.83163127  -8.10064997]\n",
            " [ -8.20242808   9.05286833   2.34141776  33.55854533 -15.0688073\n",
            "    7.24909553   5.51680729 -12.12702316  13.46433985 -17.35249231]\n",
            " [  0.88129497  -9.42206278   2.55010774 -15.0688073   25.09520869\n",
            "  -19.11763763  -0.84232143  14.62975417 -12.42517923  12.85064402]\n",
            " [  9.93497783  10.01067292  -2.19056391   7.24909553 -19.11763763\n",
            "   40.33616402  -1.35556669  -4.47334162   2.46579406 -17.19876539]\n",
            " [ -4.1667451    2.22609494   1.24388597   5.51680729  -0.84232143\n",
            "   -1.35556669  15.27292495  -0.25234391  -0.47652187  -1.80625979]\n",
            " [ 11.77262839   2.24571014  -5.38700775 -12.12702316  14.62975417\n",
            "   -4.47334162  -0.25234391  34.47544497 -15.19059165  13.82775431]\n",
            " [ -2.45207175   6.63538105  -5.83163127  13.46433985 -12.42517923\n",
            "    2.46579406  -0.47652187 -15.19059165  22.36770757  -7.89202469]\n",
            " [ -1.69033975  -6.7975317   -8.10064997 -17.35249231  12.85064402\n",
            "  -17.19876539  -1.80625979  13.82775431  -7.89202469  47.35730731]]\n"
          ]
        }
      ],
      "source": [
        "z_average = zs_test.mean(axis=0)\n",
        "z_covariance = np.cov(zs_test, rowvar=False)\n",
        "\n",
        "print(\"Average of latent codes:\", z_average)\n",
        "print(\"Covariance matrix of latent codes:\", z_covariance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhXU8cnTZ0E8"
      },
      "source": [
        "Now, generate some samples with this distribution. In this case, you actually have to calculate the Cholesky decomposition and find $\\bf{L}$. For this, you can use `np.linalg.cholesky`. Then compute the latent codes according to $z = \\mu + {\\bf{L}} \\varepsilon$.\n",
        "\n",
        "__Hint__. You can use `torch.matmul`. Pay attention to the dimension of `epsilon` to implement it correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "rSbYZLZGrcdR"
      },
      "outputs": [],
      "source": [
        "# calculate Cholesky decomposition of covariance matrix : C = L L^T\n",
        "L = np.linalg.cholesky(z_covariance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "zXGlJTZ7Z4ed"
      },
      "outputs": [],
      "source": [
        "def generate_images_non_diagonal_gaussian(ae_model, z_average, L, n_images = 5):\n",
        "\n",
        "  # Generate noise according to a standard Gaussian distribution\n",
        "  epsilon = torch.randn(n_images, z_average.shape[0])\n",
        "\n",
        "  # Sample latent codes using epsilon\n",
        "  z_average = torch.from_numpy(z_average).float()\n",
        "  L = torch.from_numpy(L).float()\n",
        "  z_generated = z_average + torch.matmul(L, epsilon.t()).t()\n",
        "  \n",
        "  # Decode back to image space\n",
        "  imgs_generated = ae_model.decoder(z_generated.to(device))\n",
        "\n",
        "  return imgs_generated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs0o_VUvR3jl"
      },
      "source": [
        "Generate images using this model now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "kGCJVjbXsKLE"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "imgs_generated = generate_images_non_diagonal_gaussian(ae_model, z_average, L, n_images = 5)\n",
        "display_images(imgs_generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLtsdri6zKEm"
      },
      "source": [
        "You should see some improvement, but we can do better than this. Thus, we turn to the variational autoencoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UqeNhuSdnDt"
      },
      "source": [
        "# 3. Variational autoencoder\n",
        "\n",
        "Now, we are going to create a variational autoencoder to carry out __image generation__. Let's first recall the idea of a variational autoencoder.\n",
        "\n",
        "### Main idea\n",
        "\n",
        "The main idea is to create an autoencoder whose latent codes follow a certain distribution (a Gaussian distribution in practice). This is done with two tools :\n",
        "\n",
        "- A specific architecture, where the encoder produces the mean and variance of the latent codes\n",
        "- A specially designed loss function\n",
        "\n",
        "Once the VAE is trained, it is possible to sample in the latent space by producing random normal variables and simply decoding.\n",
        "\n",
        "### Architecture\n",
        "\n",
        "The architecture of the VAE model is the same as before (using `Encoder` with `multiplier=2` and `Decoder`). However the wrapper `VAEModel` will be a bit more complex as we need to implement the reparametrization trick. We will also implement the code to generate samples (for test time).\n",
        "\n",
        "### Variational Autoencoder loss\n",
        "\n",
        "The VAE loss consists in a reconstruction loss and a KL divergence term.\n",
        "\n",
        "- The reconstruction loss is the same `reconstruction_loss` as before. In other words, the reconstructions are compared to the input images using binary cross-entropy. The reconstructions are generated by sampling a latent code from $q(z|x)$ and decoding it back to image space.\n",
        "\n",
        "- You will implement the KL divergence term manually below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "6siMHQLheM4T"
      },
      "outputs": [],
      "source": [
        "class VAEModel(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(VAEModel, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        self.encoder = Encoder(latent_dim, multiplier = 2)\n",
        "        self.decoder = Decoder(latent_dim)\n",
        "\n",
        "    def reparameterize(self, mean, logvar, mode='sample'):\n",
        "        \"\"\"\n",
        "        Samples from a normal distribution using the reparameterization trick.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        mean : torch.Tensor\n",
        "            Mean of the normal distribution. Shape (batch_size, latent_dim)\n",
        "\n",
        "        logvar : torch.Tensor\n",
        "            Diagonal log variance of the normal distribution. Shape (batch_size,\n",
        "            latent_dim)\n",
        "\n",
        "        mode : 'sample' or 'mean'\n",
        "            Returns either a sample from qzx, or just the mean of qzx. The former\n",
        "            is useful at training time. The latter is useful at inference time as\n",
        "            the mean is usually used for reconstruction, rather than a sample.\n",
        "        \"\"\"\n",
        "        if mode=='sample':\n",
        "            # Implements the reparametrization trick (slide 43):\n",
        "            std = torch.exp(0.5*logvar)\n",
        "            eps = torch.randn_like(std)\n",
        "            return mean + eps*std\n",
        "        elif mode=='mean':\n",
        "            return mean\n",
        "        else:\n",
        "            return ValueError(\"Unknown mode: {mode}\".format(mode))\n",
        "\n",
        "    def forward(self, x, mode='sample'):\n",
        "        \"\"\"\n",
        "        Forward pass of model, used for training or reconstruction.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Batch of data. Shape (batch_size, n_chan, height, width)\n",
        "\n",
        "        mode : 'sample' or 'mean'\n",
        "            Reconstructs using either a sample from qzx or the mean of qzx\n",
        "        \"\"\"\n",
        "\n",
        "        # stats_qzx is the output of the encoder\n",
        "        stats_qzx = self.encoder(x)\n",
        "\n",
        "        # Use the reparametrization trick to sample from q(z|x)\n",
        "        samples_qzx = self.reparameterize(*stats_qzx.unbind(-1), mode=mode)\n",
        "\n",
        "        # Decode the samples to image space\n",
        "        reconstructions = self.decoder(samples_qzx)\n",
        "\n",
        "        # Return everything:\n",
        "        return {\n",
        "            'reconstructions': reconstructions,\n",
        "            'stats_qzx': stats_qzx,\n",
        "            'samples_qzx': samples_qzx}\n",
        "\n",
        "    def sample_qzx(self, x):\n",
        "        \"\"\"\n",
        "        Returns a sample z from the latent distribution q(z|x).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Batch of data. Shape (batch_size, n_chan, height, width)\n",
        "        \"\"\"\n",
        "        stats_qzx = self.encoder(x)\n",
        "        samples_qzx = self.reparameterize(*stats_qzx.unbind(-1))\n",
        "        return samples_qzx\n",
        "\n",
        "    def sample_pz(self, N):\n",
        "        samples_pz = torch.randn(N, self.latent_dim, device=self.encoder.conv1.weight.device)\n",
        "        return samples_pz\n",
        "\n",
        "    def generate_samples(self, samples_pz=None, N=None):\n",
        "        if samples_pz is None:\n",
        "            if N is None:\n",
        "                return ValueError(\"samples_pz and N cannot be set to None at the same time. Specify one of the two.\")\n",
        "\n",
        "            # If samples z are not provided, we sample N samples from the prior\n",
        "            # p(z)=N(0,Id), using sample_pz\n",
        "            samples_pz = self.sample_pz(N)\n",
        "\n",
        "        # Decode the z's to obtain samples in image space (here, probability\n",
        "        # maps which can later be sampled from or thresholded)\n",
        "        generations = self.decoder(samples_pz)\n",
        "        return {'generations': generations}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUsrzszm-Hnf"
      },
      "source": [
        "The KL divergence term is computed as per the regularization term in slide 45 i.e., for each data sample in the mini-batch:\n",
        "$$\\frac{1}{2}\\sum_{j=1}^D (\\mu_j^2 + \\sigma_j^2 - 1 - \\log{\\sigma_j^2})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "-pc40PPM7adL"
      },
      "outputs": [],
      "source": [
        "def kl_normal_loss(mean, logvar):\n",
        "    \"\"\"\n",
        "    Calculates the KL divergence between a normal distribution\n",
        "    with diagonal covariance and a unit normal distribution.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    mean : torch.Tensor\n",
        "        Mean of the normal distribution. Shape (batch_size, latent_dim) where\n",
        "        D is dimension of distribution.\n",
        "\n",
        "    logvar : torch.Tensor\n",
        "        Diagonal log variance of the normal distribution. Shape (batch_size,\n",
        "        latent_dim)\n",
        "    \"\"\"\n",
        "    # To be consistent with the reconstruction loss, wetake the mean over the\n",
        "    # minibatch (i.e., compute for each sample in the minibatch according to\n",
        "    # the equation above, then take the mean).\n",
        "    latent_kl = (-0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp(), dim=1)).mean()\n",
        "\n",
        "    return latent_kl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhJbXJ0y_8OI"
      },
      "source": [
        "The `BetaVAELoss` puts it all together as per slide 55."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "x_hr2EwiCRSv"
      },
      "outputs": [],
      "source": [
        "class BetaVAELoss(object):\n",
        "    \"\"\"\n",
        "    Compute the Beta-VAE loss\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        beta: (scalar) the weight assigned to the regularization term\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, beta):\n",
        "        self.beta = beta\n",
        "\n",
        "    def __call__(self, reconstructions, data, stats_qzx):\n",
        "        stats_qzx = stats_qzx.unbind(-1)\n",
        "\n",
        "        # Reconstruction loss\n",
        "        rec_loss = reconstruction_loss(reconstructions, data)\n",
        "\n",
        "        # KL loss\n",
        "        kl_loss = kl_normal_loss(*stats_qzx)\n",
        "\n",
        "        # Total loss of beta-VAE\n",
        "        loss = rec_loss + self.beta * kl_loss\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RGTCPZXWaz6"
      },
      "source": [
        "### Training the Variational Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk_9fDIphlsi"
      },
      "source": [
        "This follows the traditional pipeline that you are by now familiar with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "bOdRTeDAMJCO"
      },
      "outputs": [],
      "source": [
        "latent_dim = 10\n",
        "\n",
        "learning_rate = 1e-3\n",
        "n_epoch = 5 # use the same number of epochs as before for fairness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "xGN3jpxqOOwg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "D5XnxIE1L1bI"
      },
      "outputs": [],
      "source": [
        "vae_model = VAEModel(latent_dim = latent_dim)\n",
        "vae_model = vae_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "fBVo5s-dQCBb"
      },
      "outputs": [],
      "source": [
        "# To keep it simple, we can leave beta at 1.0 for the beta-VAE loss\n",
        "# Feel free to experiment with it to see different trade-offs between reconstruction\n",
        "# and generation performance.\n",
        "\n",
        "vae_loss = BetaVAELoss(beta=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "GiqsBcP7KIT3"
      },
      "outputs": [],
      "source": [
        "# AdamW, with learning rate set to the parameter above and weight decay to 1e-4\n",
        "optimizer = optim.AdamW(vae_model.parameters(), lr=learning_rate, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "z4KHRufgxNmR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 938/938 [01:15<00:00, 12.35batch/s, loss=142]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Train Loss: 174.9510\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 938/938 [01:17<00:00, 12.06batch/s, loss=122]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 127.2217\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 938/938 [01:22<00:00, 11.42batch/s, loss=123]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Train Loss: 119.0011\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 938/938 [01:20<00:00, 11.69batch/s, loss=120]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Train Loss: 114.8839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 938/938 [01:17<00:00, 12.05batch/s, loss=112]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Train Loss: 112.6727\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "vae_model.train()\n",
        "\n",
        "for epoch in range(0,n_epoch):\n",
        "  train_loss=0.0\n",
        "\n",
        "  with tqdm(mnist_train_loader, unit=\"batch\") as tepoch:\n",
        "    for data, labels in tepoch:\n",
        "      tepoch.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "      # Put data on correct device, GPU or CPU\n",
        "      data = data.to(device)\n",
        "\n",
        "      # Pass the input data through the model\n",
        "      predict = vae_model(data)\n",
        "      reconstructions = predict['reconstructions']\n",
        "      stats_qzx = predict['stats_qzx']\n",
        "\n",
        "      # Compute the beta-VAE loss\n",
        "      loss = vae_loss(reconstructions, data, stats_qzx)\n",
        "\n",
        "      # Backpropagate\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Aggregate the training loss for display at the end of the epoch\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # tqdm bar displays the loss\n",
        "      tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "  print('Epoch {}: Train Loss: {:.4f}'.format(epoch, train_loss/len(mnist_train_loader)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DFW0vPRXrSB"
      },
      "source": [
        "### Testing the VAE model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Who7o-hAXLhl"
      },
      "source": [
        "Let's check how well the VAE reconstructs training samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "NiRg43Bgx_MH"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# reconstructing training images\n",
        "train_imgs = next(iter(mnist_train_loader))[0]\n",
        "display_ae_images(vae_model, train_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqa9wmZ3XSxD"
      },
      "source": [
        "Same for test samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "IRJ9-V0tx_MJ"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# reconstructing test images\n",
        "test_imgs = next(iter(mnist_test_loader))[0]\n",
        "display_ae_images(vae_model, test_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR-uGTX2XaEB"
      },
      "source": [
        "Let's compute the quantitative `reconstruction_loss` on the test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "P0EzdVDzx_MJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 157/157 [00:06<00:00, 25.59batch/s, loss=106] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 110.9743\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# !!! If you copy paste code, don't forget to change ae_model to vae_model !!!\n",
        "\n",
        "# FILL IN CODE\n",
        "\n",
        "vae_model.eval()\n",
        "test_loss=0.0\n",
        "\n",
        "n = 0\n",
        "with tqdm(mnist_test_loader, unit=\"batch\") as tepoch:\n",
        "  for data, labels in tepoch:\n",
        "    # Put the data on the correct device:\n",
        "    data = data.to(device)\n",
        "\n",
        "    # Pass the data through the model\n",
        "    predict = vae_model(data)\n",
        "    reconstructions = predict['reconstructions']\n",
        "    stats_qzx = predict['stats_qzx']\n",
        "\n",
        "    # Compute the beta-VAE loss\n",
        "    loss = vae_loss(reconstructions, data, stats_qzx)\n",
        "\n",
        "    # Compute the loss\n",
        "    test_loss += loss.item()\n",
        "\n",
        "    # tqdm bar displays the loss\n",
        "    tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "    # increment n to fill next parts of the arrays\n",
        "    n += minibatch_size\n",
        "\n",
        "print('Test Loss: {:.4f}'.format(test_loss/len(mnist_test_loader)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFOJ0yCzXuxO"
      },
      "source": [
        "### Image generation with the VAE model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTfRje_AkKDr"
      },
      "source": [
        "Now, generate some images with the VAE model. You can directly use the `generate_samples` routine from the `VAEModel` class above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "NGt_LSDEE2vz"
      },
      "outputs": [],
      "source": [
        "def generate_images_vae(vae_model, n_images=5):\n",
        "    return vae_model.generate_samples(N=n_images)['generations']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "41tXdNsFkKk5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "imgs_generated = generate_images_vae(vae_model)\n",
        "\n",
        "display_images(imgs_generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGnvKoynzaFN"
      },
      "source": [
        "Do you think the results are better ? What advantage does the Variational Autoencoder have over the simple autoencoder model, even though the second (non-diagonal Gaussian model) AE approach has a more complex probabilistic latent model (a full covariance matrix)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXm-D9Ef9vYm"
      },
      "source": [
        "Next, we compare the models quantitavely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04MddkzuE324"
      },
      "source": [
        "# 4. Evaluating and comparing the models for image generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "D8WsuVqmaWmd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B93T6PD2YhgF"
      },
      "source": [
        "Nowadays, the standard metric to evaluate the generative performance of a model is the FID (Fréchet Inception Distance). I leave it for you as optional homework to implement it if you wish to do so.\n",
        "\n",
        "Here, we will follow another path, a simplified version of the Inception Score (IS) that has been somewhat superseded by the FID:\n",
        "\n",
        "- we train a simple convolutional neural network classifier on MNIST, to a good accuracy\n",
        "- we generate images with each model\n",
        "- we find the average of the highest probability of the images according to the classifier, for each model. If this value is high, it means that on average the classifier considers that the images looks like a real image of an actual digit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk1OdN6jZZf1"
      },
      "source": [
        "We will use the following simple convolutional architecture for the classifier:\n",
        "\n",
        "- conv2d, 3x3 kernel, 32 filters, stride=2, padding=1; + ReLU + BatchNorm2d\n",
        "- conv2d, 3x3 kernel, 64 filters, stride=2, padding=1; + ReLU + BatchNorm2d\n",
        "- conv2d, 3x3 kernel, 128 filters, stride=2, padding=1; + ReLU + BatchNorm2d\n",
        "- Global Average Pooling\n",
        "- Flatten\n",
        "- Dense layer\n",
        "\n",
        "Now, define the model. To make things easier, we use the `torch.nn.Sequential` API to implement the model (there is no need for a Class in this simple case).\n",
        "\n",
        "__Hint__. For the global average pooling, use the `torch.nn.AvgPool2d` layer with the suitable kernel size and stride."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "P87a-DkXFOCv"
      },
      "outputs": [],
      "source": [
        "nb_classes = 10\n",
        "kernel_size = (3, 3)\n",
        "\n",
        "mnist_classification_model = torch.nn.Sequential(\n",
        "    # 1st convolutional layer\n",
        "    torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=kernel_size, stride=2, padding=1),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.BatchNorm2d(32),\n",
        "\n",
        "    # 2nd convolutional layer\n",
        "    torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=kernel_size, stride=2, padding=1),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.BatchNorm2d(64),\n",
        "\n",
        "    # 3rd convolutional layer\n",
        "    torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=kernel_size, stride=2, padding=1),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.BatchNorm2d(128),\n",
        "\n",
        "    # Global Average Pooling\n",
        "    torch.nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
        "\n",
        "    torch.nn.Flatten(),\n",
        "\n",
        "    # 10 classes\n",
        "    torch.nn.Linear(128, nb_classes)\n",
        "    \n",
        "    )\n",
        "\n",
        "mnist_classification_model = mnist_classification_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "rJ6ZYkKVaY64"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-2\n",
        "n_epoch = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Zexnteq71bJ5"
      },
      "outputs": [],
      "source": [
        "# Cross entropy with reduction='sum'\n",
        "criterion = torch.nn.CrossEntropyLoss(reduction='sum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "pWkFh6zHajBg"
      },
      "outputs": [],
      "source": [
        "# AdamW, weight decay set to 1e-4\n",
        "optimizer = torch.optim.AdamW(mnist_classification_model.parameters(), lr=learning_rate, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "eBZrmVV42Gej"
      },
      "outputs": [],
      "source": [
        "def vector_to_class(x):\n",
        "  y = torch.argmax(F.softmax(x,dim=1),axis=1)\n",
        "  return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "aOww0ydr2fT0"
      },
      "outputs": [],
      "source": [
        "def cnn_accuracy(x_pred,x_label):\n",
        "  acc = (x_pred == x_label).sum()/(x_pred.shape[0])\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "0FA8YoX2FcHP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 938/938 [01:03<00:00, 14.76batch/s, loss=0.461]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:0 Train Loss:0.1665 Accuracy:0.9499\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 938/938 [01:01<00:00, 15.23batch/s, loss=0.051]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1 Train Loss:0.0632 Accuracy:0.9805\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 938/938 [01:00<00:00, 15.44batch/s, loss=1.17]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2 Train Loss:0.0464 Accuracy:0.9860\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 938/938 [01:12<00:00, 12.90batch/s, loss=2.51]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:3 Train Loss:0.0384 Accuracy:0.9875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 938/938 [01:01<00:00, 15.32batch/s, loss=0.167] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:4 Train Loss:0.0348 Accuracy:0.9884\n"
          ]
        }
      ],
      "source": [
        "mnist_classification_model.train()\n",
        "\n",
        "for epoch in range(0,n_epoch):\n",
        "  train_loss=0.0\n",
        "  all_labels = []\n",
        "  all_predicted = []\n",
        "\n",
        "  with tqdm(mnist_train_loader, unit=\"batch\") as tepoch:\n",
        "    for imgs, labels in tepoch:\n",
        "      tepoch.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "      # Put data on correct device\n",
        "      imgs = imgs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # forward pass and loss computation\n",
        "      predict = mnist_classification_model(imgs)\n",
        "      loss = criterion(predict, labels)\n",
        "\n",
        "      # backpropagation\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # compute the train loss\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # store labels and class predictions\n",
        "      all_labels.extend(labels.tolist())\n",
        "      all_predicted.extend(vector_to_class(predict).tolist())\n",
        "\n",
        "      # tqdm bar displays the loss\n",
        "      tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "  print('Epoch:{} Train Loss:{:.4f} Accuracy:{:.4f}'.format(epoch,train_loss/len(mnist_train_loader.dataset),\n",
        "                                                            cnn_accuracy(np.array(all_predicted),np.array(all_labels))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "5bRM-1_x4bu2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 157/157 [00:04<00:00, 31.46batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Accuracy: 0.9857\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "mnist_classification_model.eval()\n",
        "\n",
        "all_predicted = []\n",
        "all_labels = []\n",
        "\n",
        "with tqdm(mnist_test_loader, unit=\"batch\") as tepoch:\n",
        "  for imgs, labels in tepoch:\n",
        "    all_labels.extend(labels.tolist())\n",
        "\n",
        "    imgs = imgs.to(device)\n",
        "    predict=mnist_classification_model(imgs)\n",
        "    all_predicted.extend(vector_to_class(predict).tolist())\n",
        "\n",
        "test_accuracy = cnn_accuracy(np.array(all_predicted),np.array(all_labels))\n",
        "\n",
        "print(\"\\nTest Accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFgN5LblFwTa"
      },
      "source": [
        "### Evaluate the average maximum prediction of the images generated by each generative model (higher is better)\n",
        "\n",
        "Now, we evaluate the models. For each ones, produce a certain number of images, and put those images through the classification network. Then find the maximum class probability of each image, and average it over all the images. We will use this as a metric to evaluate each model.\n",
        "\n",
        "__CAREFUL__: the output of the network does __not__ include the Softmax layer, so you will have to carry it out, with:\n",
        "- ```torch.nn.Softmax()(...)```\n",
        "\n",
        "Define this metric now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "lCJ_0qqjOXHT"
      },
      "outputs": [],
      "source": [
        "def generative_model_score(imgs_in, classification_model):\n",
        "    # Model in evaluation mode\n",
        "    classification_model.eval()\n",
        "\n",
        "    # Logits for the input images\n",
        "    logits = classification_model(imgs_in)\n",
        "\n",
        "    # Softmax probabilities\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "\n",
        "    # Maximum probability for each image\n",
        "    max_probs = torch.max(probs, dim=1).values\n",
        "\n",
        "    # Compute the generative model score\n",
        "    gen_score = torch.mean(max_probs)\n",
        "    \n",
        "    return gen_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGq7YFg51UoP"
      },
      "source": [
        "Now, generate some images with each of the three models, and evaluate these models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "4-L4u2jhILFx"
      },
      "outputs": [],
      "source": [
        "# Running several tests to compare the generative model score of the three models\n",
        "\n",
        "avg_diagonal_gaussian_score = 0\n",
        "avg_non_diagonal_gaussian_score = 0\n",
        "avg_vae_score = 0\n",
        "\n",
        "for i in range(50):\n",
        "    imgs_diagonal_gaussian = generate_images_diagonal_gaussian(ae_model,z_average,z_sigma,n_images = 2000)\n",
        "    imgs_non_diagonal_gaussian = generate_images_non_diagonal_gaussian(ae_model,z_average,L,n_images = 2000)\n",
        "    imgs_vae = generate_images_vae(vae_model,n_images=2000)\n",
        "\n",
        "    # average of maximum of first model\n",
        "    diagonal_gaussian_score = float(generative_model_score(imgs_diagonal_gaussian,mnist_classification_model))\n",
        "    non_diagonal_gaussian_score = float(generative_model_score(imgs_non_diagonal_gaussian,mnist_classification_model))\n",
        "    vae_gaussian_score = float(generative_model_score(imgs_vae,mnist_classification_model))\n",
        "\n",
        "    avg_diagonal_gaussian_score += diagonal_gaussian_score\n",
        "    avg_non_diagonal_gaussian_score += non_diagonal_gaussian_score\n",
        "    avg_vae_score += vae_gaussian_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diagonal gaussian generative model score :  0.8715170013904572\n",
            "Non diagonal gaussian generative model score :  0.8915362656116486\n",
            "Variational autoencoder model score:  0.904859230518341\n"
          ]
        }
      ],
      "source": [
        "print(\"Diagonal gaussian generative model score : \", avg_diagonal_gaussian_score / 50)\n",
        "print(\"Non diagonal gaussian generative model score : \", avg_non_diagonal_gaussian_score / 50)\n",
        "print(\"Variational autoencoder model score: \", avg_vae_score / 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxvsG8FC1gNS"
      },
      "source": [
        "**Questions:**\n",
        "\n",
        "- Which model is better quantitatively ? (unfortunately there is some variability, even with 2000 samples; you might want to rerun the cell several times to get the trend)\n",
        "\n",
        "    The Variational Autoencoder (VAE) model achieves the highest score of 0.904859230518341. This indicates that quantitatively, the VAE model is superior among the three tested models.\n",
        "\n",
        "    The scores reflect the classifier’s confidence in the images generated by each model, suggesting that the VAE produces images that are more 'true' or similar to the original data distribution seen during the classifier’s training. The differences in scores, while not massive, are significant enough to suggest that the complexity and flexibility in the VAE’s structure allow it to model the data distribution more accurately than Gaussian models with either diagonal or non-diagonal covariance structures.\n",
        "\n",
        "    The VAE's superior performance is typically attributed to its ability to learn a more complex latent space representation, giving it a significant advantage in tasks that require modeling complex distributions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Do the quantitative result support the qualitative results ?\n",
        "\n",
        "    The quantitative results do seem to support the qualitative findings, albeit not with a stark difference.\n",
        "\n",
        "    The higher scores of the Variational Autoencoder (VAE) quantitatively suggest better performance in generating images that the classification model recognizes with higher confidence. This aligns with the observation that VAE-generated images appear clearer and more recognizable compared to those from the other models.\n",
        "\n",
        "    The VAE's ability to generate clearer and more recognizable digits from MNIST is consistent with its higher generative model score. This suggests that the VAE is indeed producing images closer to the real data distribution of MNIST, which are easier for the classifier to interpret.\n",
        "\n",
        "    The Gaussian models producing blurrier and sometimes unrecognizable images and receiving lower scores also match up. If the classifier struggles to recognize an image, it indicates that these models may not capture the data's underlying structure as effectively as the VAE, reflecting in both lower quantitative scores and poorer image quality.\n",
        "\n",
        "    Although the overall quality across the three models is relatively similar, even slight improvements in clarity and recognizability can lead to better quantitative scores. This is because the classification model likely uses detailed features of the digits to make predictions, and even small improvements in these features' visibility can significantly affect performance scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Can you see any drawbacks of this method of evaluation ?\n",
        "\n",
        "    The evaluation heavily relies on the performance of the classification model. If the classifier is not robust or well-trained on a diverse set of real images, it might not accurately assess the quality of the generated images. Misclassifications due to classifier limitations could be misinterpreted as flaws in the generative model.\n",
        "\n",
        "    The method focuses on how well the generated images are recognized by a specific classifier, which might not fully represent the overall quality of the generated images. For example, the classifier might recognize a blurry but structurally correct digit, even though such images might not be desirable in applications requiring high image clarity.\n",
        "\n",
        "    The classifier’s ability to recognize images is based on its training on the original dataset. This can bias the evaluation towards images that closely mimic the training data, potentially penalizing generative models that produce novel or varied outputs that are still valid but not represented in the training dataset.\n",
        "\n",
        "    The evaluation might not effectively detect mode collapse, a common problem in generative models where they produce a limited variety of outputs. If the model repeatedly generates a few types of images very well, the classifier’s high confidence scores might give a misleading impression of overall model performance.\n",
        "\n",
        "    The method primarily assesses the accuracy of individual images and does not account for the diversity or novelty of the images generated by the model. A generative model could score highly by producing very accurate but non-diverse images, which would not be desirable for applications requiring a rich variety of outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Can you propose any more sophisticated models than the multivariate Gaussian approach (apart from the variational autoencoder) ?\n",
        "\n",
        "    1. **Generative Adversarial Networks (GANs)**:\n",
        "    - GANs consist of two neural networks, the generator and the discriminator, which are trained in opposition to each other. The generator learns to produce increasingly realistic images, while the discriminator learns to distinguish between real and generated images.\n",
        "    - GANs are known for producing high-quality and highly realistic images. They have been successful in a variety of tasks, including image super-resolution, style transfer, and more.\n",
        "    - Training GANs can be difficult due to issues like mode collapse (where the generator produces limited varieties of outputs) and training instability.\n",
        "\n",
        "    2. **Autoregressive Models (e.g., PixelRNN, PixelCNN)**:\n",
        "    - These models generate images pixel by pixel, conditioning each pixel on the previously generated pixels. This sequential generation process allows for precise modeling of the data distribution.\n",
        "    - They excel in modeling the joint distribution of the pixels and can generate highly detailed images with coherent structures.\n",
        "    - The sequential nature of generation can lead to slower sample generation compared to parallel methods like VAEs and GANs."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
